<!DOCTYPE html>
<html lang="ar">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>نظارة AR بتقنية التعرف على الوجه</title>

    <!-- مكتبة Three.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/three/examples/js/loaders/GLTFLoader.js"></script>

    <!-- مكتبة face-api.js -->
    <script defer src="https://cdn.jsdelivr.net/npm/face-api.js"></script>

    <style>
        body {
            margin: 0;
            overflow: hidden;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            height: 100vh;
            background-color: #000;
        }
        canvas {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
        }
        video {
            display: none;
            position: absolute;
            width: 100%;
            height: 100%;
            object-fit: cover;
            opacity: 0.8;
        }
        .controls {
            position: absolute;
            bottom: 20px;
            display: flex;
            gap: 10px;
        }
        button {
            padding: 10px 20px;
            font-size: 16px;
            background-color: #007BFF;
            color: white;
            border: none;
            border-radius: 5px;
            cursor: pointer;
        }
        button:hover {
            background-color: #0056b3;
        }
    </style>
</head>
<body>

    <video id="video" autoplay playsinline></video>
    <canvas id="output"></canvas>

    <!-- أزرار التحكم -->
    <div class="controls">
        <button id="showModel">عرض النموذج</button>
        <button id="showCamera" style="display:none;">عرض الكاميرا</button>
        <button id="startAR" style="display:none;">تجربة</button>
    </div>

    <script>
        let scene, camera, renderer, model, video, canvas, ctx;
        let isModelLoaded = false;
        let isFaceDetectionRunning = false;

        function initScene() {
            scene = new THREE.Scene();
            camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);
            camera.position.z = 2;
            
            renderer = new THREE.WebGLRenderer({ alpha: true });
            renderer.setSize(window.innerWidth, window.innerHeight);
            document.body.appendChild(renderer.domElement);
        }

        function loadModel() {
            if (isModelLoaded) return;

            const loader = new THREE.GLTFLoader();
            loader.load('./glass1.glb', (gltf) => {
                model = gltf.scene;
                model.scale.set(0.3, 0.3, 0.3);
                model.position.set(0, 0, 0);
                scene.add(model);
                isModelLoaded = true;

                document.getElementById("showCamera").style.display = "block"; // إظهار زر الكاميرا بعد تحميل النموذج
            }, undefined, (error) => {
                console.error('❌ خطأ في تحميل النموذج:', error);
            });
        }

        function startCamera() {
            video = document.getElementById('video');
            canvas = document.getElementById('output');
            ctx = canvas.getContext('2d');

            navigator.mediaDevices.getUserMedia({ video: { facingMode: "user" } })
                .then((stream) => {
                    video.srcObject = stream;
                    video.style.display = "block";
                    document.getElementById("startAR").style.display = "block"; // إظهار زر "تجربة"
                })
                .catch((error) => console.error("❌ فشل في الوصول إلى الكاميرا:", error));
        }

        async function startFaceTracking() {
            if (isFaceDetectionRunning) return;
            isFaceDetectionRunning = true;

            await Promise.all([
                faceapi.nets.ssdMobilenetv1.loadFromUri('https://cdn.jsdelivr.net/npm/face-api.js/weights'),
                faceapi.nets.faceLandmark68Net.loadFromUri('https://cdn.jsdelivr.net/npm/face-api.js/weights')
            ]);

            async function detectFace() {
                const predictions = await faceapi.detectSingleFace(video).withFaceLandmarks();
                if (predictions) {
                    const keypoints = predictions.landmarks.positions;
                    const leftEye = keypoints[33];
                    const rightEye = keypoints[133];
                    const nose = keypoints[168];

                    if (model) {
                        const eyeDistance = Math.sqrt(Math.pow(leftEye.x - rightEye.x, 2) + Math.pow(leftEye.y - rightEye.y, 2));

                        model.position.set(nose.x / 100 - 1, -nose.y / 100 + 1, -2);
                        model.scale.set(eyeDistance / 200, eyeDistance / 200, eyeDistance / 200);
                    }
                }
                requestAnimationFrame(detectFace);
            }
            detectFace();
        }

        function animate() {
            requestAnimationFrame(animate);
            renderer.render(scene, camera);
        }

        document.getElementById("showModel").addEventListener("click", () => {
            initScene();
            loadModel();
            animate();
        });

        document.getElementById("showCamera").addEventListener("click", () => {
            startCamera();
        });

        document.getElementById("startAR").addEventListener("click", () => {
            startFaceTracking();
        });
    </script>

</body>
</html>
